{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Million Song Dataset Taste Profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Taste profile dataset](http://labrosa.ee.columbia.edu/millionsong/tasteprofile) contains real user - play counts from undisclosed users, with following statistics:\n",
    "\n",
    "* 1,019,318 unique users\n",
    "* 384,546 unique MSD songs\n",
    "* 48,373,586 user - song - play count triplets\n",
    "\n",
    "This is the script that subsamples the full dataset and splits it into non-overlapping training, validation, test sets. This subset is used in the paper: [\"modeling user exposure in recommendation\"](http://arxiv.org/abs/1510.07025).\n",
    "\n",
    "This notebook was based on [this one](https://github.com/dawenl/expo-mf/blob/master/src/processTasteProfile.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sqlite3\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to wherever you keep the data\n",
    "TPS_DIR = '/home/cfragada/source/Postdoc/KL_screening/datasets/Recommendation/tasteprofile' #'/home/waldorf/dawen.liang/data/tasteprofile/'\n",
    "\n",
    "# The dataset can be obtained here:\n",
    "# http://labrosa.ee.columbia.edu/millionsong/sites/default/files/challenge/train_triplets.txt.zip\n",
    "TP_file = os.path.join(TPS_DIR, 'train_triplets.txt')\n",
    "\n",
    "# track_metadata.db contains all the metadata, which is not required to subsample the data, but only used when \n",
    "# referring to the actual information about particular pieces (e.g. artist, song name, etc.)\n",
    "# Available here: http://labrosa.ee.columbia.edu/millionsong/sites/default/files/AdditionalFiles/track_metadata.db\n",
    "md_dbfile = os.path.join(TPS_DIR, 'track_metadata.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tp = pd.read_table(TP_file, header=None, names=['uid', 'sid', 'count'])\n",
    "tp = pd.read_csv(TP_file, header=None, names=['uid', 'sid', 'count'], sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out \"inactive\" users and songs\n",
    "* Only keep the users who listened to at least 20 songs and the songs that are listened to by at least 50 users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the user-playcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only keep songs that are listened to by at least MIN_SONG_COUNT users and users who have listened \n",
    "# to at least MIN_USER_COUNT songs\n",
    "\n",
    "MIN_USER_COUNT = 20\n",
    "MIN_SONG_COUNT = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(tp, id):\n",
    "    playcount_groupbyid = tp[[id, 'count']].groupby(id, as_index=False)\n",
    "    count = playcount_groupbyid.size()\n",
    "    return count\n",
    "\n",
    "def filter_triplets(tp, min_uc=MIN_USER_COUNT, min_sc=MIN_SONG_COUNT):\n",
    "    # Only keep the triplets for songs which were listened to by at least min_sc users. \n",
    "    songcount = get_count(tp, 'sid')\n",
    "    tp = tp[tp['sid'].isin(songcount.index[songcount >= min_sc])]\n",
    "    \n",
    "    # Only keep the triplets for users who listened to at least min_uc songs\n",
    "    # After doing this, some of the songs will have less than min_uc users, but should only be a small proportion\n",
    "    usercount = get_count(tp, 'uid')\n",
    "    tp = tp[tp['uid'].isin(usercount.index[usercount >= min_uc])]\n",
    "    \n",
    "    # Update both usercount and songcount after filtering\n",
    "    usercount, songcount = get_count(tp, 'uid'), get_count(tp, 'sid') \n",
    "    return tp, usercount, songcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, usercount, songcount = filter_triplets(tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering, there are 39730795 triplets from 629112 users and 98485 songs (sparsity level 0.064%)\n"
     ]
    }
   ],
   "source": [
    "sparsity_level = float(tp.shape[0]) / (usercount.shape[0] * songcount.shape[0])\n",
    "print \"After filtering, there are %d triplets from %d users and %d songs (sparsity level %.3f%%)\" % (tp.shape[0], \n",
    "                                                                                                      usercount.shape[0], \n",
    "                                                                                                      songcount.shape[0], \n",
    "                                                                                                      sparsity_level * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    # Save original song and user identifiers\n",
    "    with open(os.path.join(TPS_DIR, 'filtered_uid.txt'), 'w') as f:\n",
    "        for uid in usercount.index:\n",
    "            f.write('%s\\n' % uid)\n",
    "    with open(os.path.join(TPS_DIR, 'filtered_sid.txt'), 'w') as f:\n",
    "        for sid in songcount.index:\n",
    "            f.write('%s\\n' % sid)        \n",
    "\n",
    "        \n",
    "    # Replace song and user identifiers by numbers\n",
    "    song2id = dict((sid, i) for (i, sid) in enumerate(songcount.index))\n",
    "    user2id = dict((uid, i) for (i, uid) in enumerate(usercount.index))\n",
    "\n",
    "    def numerize(tp):\n",
    "        uid = map(lambda x: user2id[x], tp['uid'])\n",
    "        sid = map(lambda x: song2id[x], tp['sid'])\n",
    "        tp['uid'] = uid\n",
    "        tp['sid'] = sid\n",
    "        return tp\n",
    "\n",
    "    tp_copy = tp.copy()\n",
    "\n",
    "    numerize(tp_copy).to_csv(os.path.join(TPS_DIR, 'TasteProfile_train_triplets_filtered.csv'), index=False)\n",
    "    del tp_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsample ~N songs and ~M users:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First sample ~1.1*M users based on listening count, only keep the data with those 1.1*M users\n",
    "* Then sample ~1.1*N songs from the pre-selected user listening history based on listening count\n",
    "* Only keep the users who listened to at least 20 songs and the songs that are listened to by at least 50 users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 options for choosing the subset of users: \n",
    "#  1) keep_max = True, keeps the users and songs with HIGHEST listening count\n",
    "#  2) keep_max = False, random with probability of picking a user or song is porportional to the listening count\n",
    "\n",
    "keep_max = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNCOMMENT the chosen subsampling rate\n",
    "\n",
    "# ~ 20k x 200k\n",
    "# mode_str = '20k200k' #(nbSongs x nbUsers)\n",
    "# if keep_max:\n",
    "#     n_users = 200000\n",
    "#     n_songs = 20000\n",
    "# else:\n",
    "#     n_users = 250000\n",
    "#     n_songs = 25000\n",
    "\n",
    "# ~ 10k x 100k\n",
    "#mode_str = '10k100k'\n",
    "# if keep_max:\n",
    "#     n_users = 100000\n",
    "#     n_songs = 10000\n",
    "# else:\n",
    "#     n_users = 110000\n",
    "#     n_songs = 11000\n",
    "\n",
    "# ~ 5k x 50k\n",
    "mode_str = '5k50k'\n",
    "if keep_max:\n",
    "    n_users = 50000\n",
    "    n_songs = 5000\n",
    "else:\n",
    "    n_users = 80000\n",
    "    n_songs = 6000\n",
    "\n",
    "np.random.seed(98765)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick n_users users\n",
    "if keep_max:\n",
    "    sorted_usercount = usercount.sort_values(ascending=False)\n",
    "    unique_uid = sorted_usercount.index[:n_users]\n",
    "else:\n",
    "    unique_uid = usercount.index\n",
    "    p_users = usercount / usercount.sum()\n",
    "    idx = np.random.choice(len(unique_uid), size=n_users, replace=False, p=p_users.tolist())\n",
    "    unique_uid = unique_uid[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = tp[tp['uid'].isin(unique_uid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick n_songs songs\n",
    "if keep_max:\n",
    "    sorted_songcount = songcount.sort_values(ascending=False)\n",
    "    unique_sid = sorted_songcount.index[:n_songs]\n",
    "else:\n",
    "    unique_sid = songcount.index\n",
    "    p_songs = songcount / songcount.sum()\n",
    "    idx = np.random.choice(len(unique_sid), size=n_songs, replace=False, p=p_songs.tolist())\n",
    "    unique_sid = unique_sid[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = tp[tp['sid'].isin(unique_sid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out users and songs below a minimum listening count\n",
    "tp, usercount, songcount = filter_triplets(tp, min_uc=20, min_sc=50)\n",
    "unique_uid = usercount.index\n",
    "unique_sid = songcount.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After subsampling and filtering, there are 2131575 triplets from 44062 users and 4835 songs (sparsity level 1.001%)\n"
     ]
    }
   ],
   "source": [
    "sparsity_level = float(tp.shape[0]) / (usercount.shape[0] * songcount.shape[0])\n",
    "print \"After subsampling and filtering, there are %d triplets from %d users and %d songs (sparsity level %.3f%%)\" % \\\n",
    "(tp.shape[0], usercount.shape[0], songcount.shape[0], sparsity_level * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save original song and user identifiers\n",
    "with open(os.path.join(TPS_DIR, 'reduced'+mode_str+'_uid.txt'), 'w') as f:\n",
    "    for uid in unique_uid:\n",
    "        f.write('%s\\n' % uid)\n",
    "        \n",
    "with open(os.path.join(TPS_DIR, 'reduced'+mode_str+'_sid.txt'), 'w') as f:\n",
    "    for sid in unique_sid:\n",
    "        f.write('%s\\n' % sid)\n",
    "\n",
    "# Replace song and user identifiers by numbers\n",
    "song2id = dict((sid, i) for (i, sid) in enumerate(unique_sid))\n",
    "user2id = dict((uid, i) for (i, uid) in enumerate(unique_uid))\n",
    "\n",
    "def numerize(tp):\n",
    "    uid = map(lambda x: user2id[x], tp['uid'])\n",
    "    sid = map(lambda x: song2id[x], tp['sid'])\n",
    "    tp['uid'] = uid\n",
    "    tp['sid'] = sid\n",
    "    return tp\n",
    "\n",
    "#tp_copy = tp.copy();\n",
    "\n",
    "if keep_max:\n",
    "    numerize(tp).to_csv(os.path.join(TPS_DIR, 'TasteProfile_train_triplets_reduced'+mode_str+'_max.csv'), index=False)\n",
    "else:\n",
    "    numerize(tp).to_csv(os.path.join(TPS_DIR, 'TasteProfile_train_triplets_reduced'+mode_str+'.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py2]",
   "language": "python",
   "name": "conda-env-py2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
